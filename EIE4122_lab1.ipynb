{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
        "- [Model Architecture](#1)\n",
        "- [Dataset](#2)\n",
        "- [Training pipeline](#3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHHd-zPzO4Eh",
        "outputId": "3e452dea-af62-45ae-ac9f-f60fcf847d29"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 3319731.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28862447.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3534298.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# generating a random seed\n",
        "torch.manual_seed(21)\n",
        "torch.cuda.manual_seed(21)\n",
        "\n",
        "# Defining Hyperparamethers\n",
        "cuda = True\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# importing the MNIST dataset\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(64*7*7, 512) \n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.bn6 = nn.BatchNorm1d(1024)\n",
        "        self.fc3 = nn.Linear(1024, 10)\n",
        "\n",
        "        # Dropout Layers\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.bn6(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Net()\n",
        "# move the model to the GPU\n",
        "if cuda: model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAB1hjMNTNVJ",
        "outputId": "970da2ef-d281-4a7f-be16-0314ba2ae042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 11, 11, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 278,858\n",
            "Trainable params: 277,834\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "\n",
        "# train the network\n",
        "def train():\n",
        "    \n",
        "    # log the training information so we can later visualise it using tensorboard\n",
        "    tb = SummaryWriter('./log_pytorch')\n",
        "\n",
        "    # put the model into training mode\n",
        "    model.train()\n",
        "    # start the training, which will run for several epochs (https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        # keep track of the number of correctly classified samples during each epoch\n",
        "        correct = 0\n",
        "        # create our function to train the model\n",
        "        # divide the dataset into batches (https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # move the data to the GPU\n",
        "            if args.cuda: data, target = data.cuda(), target.cuda()\n",
        "            # convert the data to PyTorch variables (Tensor)\n",
        "            #data, target = Variable(data), Variable(target)\n",
        "            # initialise the gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            # feed the input data into the CNN model and get the output\n",
        "            output = model(data)\n",
        "            # calculate the error by comparing the result that should be obtained with the output produced by our model\n",
        "            # here we use the negative log likelihood loss (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            # once we have measure the error, we apply back-propagation to calculate the gradients needed to update the model parameters\n",
        "            # https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "            loss.backward()\n",
        "            # update the model parameters (weights) using SGD\n",
        "            optimizer.step()\n",
        "            # the predicted class is the one with the maximum probability in the posterior probability vector (output of softmax)\n",
        "            pred = output.data.max(1)[1]\n",
        "            # count the number of times the prediction made by our model is correct (i.e., equal to the target)\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum() # M\n",
        "    \n",
        "            # print the current loss\n",
        "            if batch_idx % args.log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item())) \n",
        "                    \n",
        "        # add train loss to tensorboard\n",
        "        tb.add_scalar(\"epoch loss\", loss.item(), epoch)\n",
        "        # add accuracy to tensorboard\n",
        "        tb.add_scalar(\"epoch accuracy\", 100. * correct / len(train_loader.dataset), epoch)\n",
        "        # add weight histogram to tensorboard\n",
        "        for name, weight in model.named_parameters():\n",
        "            tb.add_histogram(name, weight, epoch)\n",
        "            tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
        "            \n",
        "    # save the model to a .pth file\n",
        "    print('Saving CNN to %s' % args.model_path)\n",
        "    torch.save(model.state_dict(), args.model_path)\n",
        "    # add graph to tensorboard\n",
        "    tb.add_graph(model, (data,))\n",
        "        \n",
        "\n",
        "# evaluate the model performance (accuracy) on the test data\n",
        "def test():\n",
        "    \n",
        "    # load the model we trained and saved in args.model_path\n",
        "    model.load_state_dict(torch.load(args.model_path))\n",
        "    \n",
        "    # put model into test mode\n",
        "    model.eval()\n",
        "    # initialise the variables used to track the loss and the correct predictions made by the model\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # fetch the test data\n",
        "    for data, target in test_loader:\n",
        "        if args.cuda: data, target = data.cuda(), target.cuda()\n",
        "        # no need to compute gradients now, we're only \"using\" the CNN, we're not training it\n",
        "        with torch.no_grad():\n",
        "            # convert the data to PyTorch variables (Tensor)\n",
        "            # data, target = Variable(data, volatile=True), Variable(target)\n",
        "            # feed the input data into the CNN model and get the output\n",
        "            output = model(data)\n",
        "            # calculate the loss for this batch of data points and add it to the total\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            # the predicted class is the one with the maximum probability in the posterior probability vector (output of softmax)\n",
        "            pred = output.data.max(1)[1]\n",
        "            # count the number of times the prediction made by our model is correct (i.e., equal to the target)\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum() # M\n",
        "\n",
        "    # calculate the average loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    # print the average loss and the percentage of correct classifications made by our model (i.e., accuracy)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100*correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FAkNcpYr9tY",
        "outputId": "e0191da3-d190-4043-a6c2-5435760249b6"
      },
      "outputs": [],
      "source": [
        "!python mnist_cnn_keras.py --mode 'test' --model_path 'models/mnist_cnn.keras'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bli-OJar9xf"
      },
      "outputs": [],
      "source": [
        "!python mni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XbXOi0OWr90K"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (976892717.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
        "- [Section 1](#1)\n",
        "- [Section 2](#2)\n",
        "- [Section 3](#3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp1d-U_vprDU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
